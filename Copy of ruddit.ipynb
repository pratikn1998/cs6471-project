{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gP82kfgyJpyF","outputId":"8e4a8a7a-3dac-4b4c-d7af-f23fa50f85ee","executionInfo":{"status":"ok","timestamp":1650156723626,"user_tz":240,"elapsed":12760,"user":{"displayName":"Roshan Konda","userId":"04517896983970965571"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.18.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.5.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.49)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: small-text[transformers]==1.0.0b3 in /usr/local/lib/python3.7/dist-packages (1.0.0b3)\n","Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.7/dist-packages (from small-text[transformers]==1.0.0b3) (1.21.5)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from small-text[transformers]==1.0.0b3) (0.3.4)\n","Requirement already satisfied: scikit-learn>=0.24.1 in /usr/local/lib/python3.7/dist-packages (from small-text[transformers]==1.0.0b3) (1.0.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from small-text[transformers]==1.0.0b3) (4.64.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from small-text[transformers]==1.0.0b3) (1.4.1)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from small-text[transformers]==1.0.0b3) (1.10.0+cu111)\n","Requirement already satisfied: transformers>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from small-text[transformers]==1.0.0b3) (4.18.0)\n","Requirement already satisfied: torchtext>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from small-text[transformers]==1.0.0b3) (0.11.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24.1->small-text[transformers]==1.0.0b3) (1.1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24.1->small-text[transformers]==1.0.0b3) (3.1.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->small-text[transformers]==1.0.0b3) (4.1.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext>=0.7.0->small-text[transformers]==1.0.0b3) (2.23.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->small-text[transformers]==1.0.0b3) (21.3)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->small-text[transformers]==1.0.0b3) (0.5.1)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->small-text[transformers]==1.0.0b3) (0.0.49)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->small-text[transformers]==1.0.0b3) (6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->small-text[transformers]==1.0.0b3) (3.6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->small-text[transformers]==1.0.0b3) (2019.12.20)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->small-text[transformers]==1.0.0b3) (4.11.3)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->small-text[transformers]==1.0.0b3) (0.12.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers>=4.0.0->small-text[transformers]==1.0.0b3) (3.0.8)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers>=4.0.0->small-text[transformers]==1.0.0b3) (3.8.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext>=0.7.0->small-text[transformers]==1.0.0b3) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext>=0.7.0->small-text[transformers]==1.0.0b3) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext>=0.7.0->small-text[transformers]==1.0.0b3) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext>=0.7.0->small-text[transformers]==1.0.0b3) (3.0.4)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.0.0->small-text[transformers]==1.0.0b3) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.0.0->small-text[transformers]==1.0.0b3) (1.15.0)\n"]}],"source":["!pip install transformers #snorkel better_profanity textblob\n","!pip install small-text[transformers]==1.0.0b3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vXk4gB0B4vdl"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from google.colab import drive\n","from nltk.corpus import stopwords\n","import nltk\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import SVC\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","import tensorflow_hub as hub\n","from gensim.models import KeyedVectors\n","from transformers import BertTokenizer\n","from transformers import BertTokenizerFast, BertForSequenceClassification\n","from transformers import BertConfig\n","from transformers import BertModel, get_linear_schedule_with_warmup\n","from transformers import RobertaTokenizer\n","from transformers import RobertaConfig\n","from transformers import RobertaModel\n","from torch.utils.data import Dataset, DataLoader\n","import torch\n","from torch import nn\n","from sklearn.metrics import accuracy_score,f1_score\n","from sklearn.metrics import confusion_matrix\n","from collections import Counter\n","import spacy\n","from keras.preprocessing import sequence\n","from keras.preprocessing.text import Tokenizer\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn.functional as F\n","from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","from keras.preprocessing import sequence\n","from tqdm import tqdm\n","from sklearn.metrics import mean_squared_error\n","from torch.optim import AdamW\n","from collections import defaultdict\n","from scipy.stats import entropy"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9Vpsu_H69a4T","outputId":"75a2b3c8-0039-4b2f-999c-f7b862f3dd1f","executionInfo":{"status":"ok","timestamp":1650156730837,"user_tz":240,"elapsed":8,"user":{"displayName":"Roshan Konda","userId":"04517896983970965571"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["nltk.download('stopwords')\n","stop_words = set(stopwords.words('english'))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MX9Oy1D64-1i","outputId":"4533d4bd-cc18-4910-8b03-54ad9045b3e1","executionInfo":{"status":"ok","timestamp":1650156731624,"user_tz":240,"elapsed":793,"user":{"displayName":"Roshan Konda","userId":"04517896983970965571"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}],"source":["drive.mount('/content/gdrive')"]},{"cell_type":"markdown","source":[""],"metadata":{"id":"aMpAeDEg8XYG"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"GWjWhM-S1jxY"},"outputs":[],"source":["#load pretrained word2vec\n","#word2vec = KeyedVectors.load_word2vec_format(\"/content/gdrive/MyDrive/CS6471/GoogleNews-vectors-negative300.bin.gz\", binary=True)"]},{"cell_type":"markdown","metadata":{"id":"tHDb3eaD6S8l"},"source":["# Data Preprocessing\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YdIiIMzc5AZc"},"outputs":[],"source":["#load data\n","df = pd.read_csv('/content/gdrive/MyDrive/CS6471/reddit_comments.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TpQs_5lC5SDF"},"outputs":[],"source":["#discretize data according to this split\n","split = [\n","         [-1,-0.34],\n","         [-0.34,0.34],\n","         [0.34,1]\n","         ]\n","\n","def discretize(val,split):\n","  if(split[0][0]<=val<split[0][1]):\n","    return 0\n","  elif(split[1][0]<=val<split[1][1]):\n","    return 1\n","  else:\n","    return 2\n","\n","\n","df['class'] = df['offensiveness_score'].apply(lambda x:discretize(x,split))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"liw0Re575Vl-"},"outputs":[],"source":["#clean text\n","whitelist = set('abcdefghijklmnopqrstuvwxyz ABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890')\n","def cleaner(x):\n","    x = x.replace(\"\\n\",\" \")\n","    x = \"\".join(filter(whitelist.__contains__, x))\n","    x = x.lower()\n","    x = x.strip()\n","    return x\n","df[\"clean_text\"] = df[\"txt\"].apply(cleaner)\n"]},{"cell_type":"markdown","metadata":{"id":"3qIrp2kRqKzO"},"source":["# Baseline Data Split"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"927I5lY6qKHH"},"outputs":[],"source":["#split df into train and test\n","df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n","y_train = list(df_train['class'])\n","y_test = list(df_test['class'])"]},{"cell_type":"markdown","metadata":{"id":"lEDGq28Vqw-Q"},"source":["# Logistic Regression, SVM, Random Forest with Tfidf, Count Vectorizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lzq1CNddtd8N"},"outputs":[],"source":["#tokenize data with tfidf\n","tfidf = TfidfVectorizer(stop_words=stop_words, max_features=1000)\n","countvec = CountVectorizer(stop_words = stop_words, max_features = 1000)\n","x_train = tfidf.fit_transform(df_train[\"clean_text\"])\n","x_test = tfidf.transform(df_test[\"clean_text\"])\n","\n","x_train_countvec = countvec.fit_transform(df_train[\"clean_text\"])\n","x_test_countvec = countvec.transform(df_test[\"clean_text\"])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kR8ZQWXWSMWf","outputId":"02840d5e-e8ca-4a18-8bc0-96d9b1f0a295","executionInfo":{"status":"ok","timestamp":1650155729809,"user_tz":240,"elapsed":691,"user":{"displayName":"Roshan Konda","userId":"04517896983970965571"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["----Count Vectorizer----\n","Accuracy 0.7689210950080515\n","F1-score 0.6214982506728941\n","[[ 45 148   0]\n"," [ 61 812  29]\n"," [  3  46  98]]\n","----TFIDF----\n","Accuracy 0.782608695652174\n","F1-score 0.5691494521430437\n","[[ 18 175   0]\n"," [ 21 873   8]\n"," [  1  65  81]]\n"]}],"source":["#Logistic regression classifying toxic and non toxic comments\n","\n","log_model = LogisticRegression()\n","log_model.fit(x_train_countvec, y_train)\n","pred_log_model = log_model.predict(x_test_countvec)\n","\n","print(\"----Count Vectorizer----\")\n","print(\"Accuracy {}\".format(accuracy_score(y_test, pred_log_model)))\n","print(\"F1-score {}\".format(f1_score(y_test, pred_log_model,average = \"macro\")))\n","print(confusion_matrix(y_test,pred_log_model))\n","\n","log_model = LogisticRegression()\n","log_model.fit(x_train, y_train)\n","pred_log_model = log_model.predict(x_test)\n","\n","print(\"----TFIDF----\")\n","print(\"Accuracy {}\".format(accuracy_score(y_test, pred_log_model)))\n","print(\"F1-score {}\".format(f1_score(y_test, pred_log_model,average = \"macro\")))\n","print(confusion_matrix(y_test,pred_log_model))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uHeQ7JhzSwPA","outputId":"843fd8e3-ba4e-4404-fa64-54dc86718ef0","executionInfo":{"status":"ok","timestamp":1650155741756,"user_tz":240,"elapsed":7686,"user":{"displayName":"Roshan Konda","userId":"04517896983970965571"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["----Count Vectorizer----\n","Accuracy 0.789049919484702\n","F1-score 0.5329316640584246\n","[[  5 188   0]\n"," [  1 899   2]\n"," [  1  70  76]]\n","----TFIDF----\n","Accuracy 0.7954911433172303\n","F1-score 0.5790083145854453\n","[[ 19 174   0]\n"," [  9 890   3]\n"," [  0  68  79]]\n"]}],"source":["#SVM\n","svm_model = SVC()\n","svm_model.fit(x_train_countvec, y_train)\n","pred_svm_model = svm_model.predict(x_test_countvec)\n","\n","print(\"----Count Vectorizer----\")\n","print(\"Accuracy {}\".format(accuracy_score(y_test, pred_svm_model)))\n","print(\"F1-score {}\".format(f1_score(y_test, pred_svm_model,average = \"macro\")))\n","print(confusion_matrix(y_test,pred_svm_model))\n","\n","svm_model = SVC()\n","svm_model.fit(x_train, y_train)\n","pred_svm_model = svm_model.predict(x_test)\n","\n","print(\"----TFIDF----\")\n","print(\"Accuracy {}\".format(accuracy_score(y_test, pred_svm_model)))\n","print(\"F1-score {}\".format(f1_score(y_test, pred_svm_model,average = \"macro\")))\n","print(confusion_matrix(y_test,pred_svm_model))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f5OPgU9US8bU","outputId":"8a19cf5c-24fd-4a22-ac49-e2466706252e","executionInfo":{"status":"ok","timestamp":1650155753759,"user_tz":240,"elapsed":5891,"user":{"displayName":"Roshan Konda","userId":"04517896983970965571"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["----Count Vectorizer----\n","Accuracy 0.7954911433172303\n","F1-score 0.6843026014669645\n","[[ 75 116   2]\n"," [ 75 816  11]\n"," [  7  43  97]]\n","----TFIDF----\n","Accuracy 0.8067632850241546\n","F1-score 0.6820117004316231\n"]},{"output_type":"execute_result","data":{"text/plain":["array([[ 60, 133,   0],\n","       [ 49, 841,  12],\n","       [  4,  42, 101]])"]},"metadata":{},"execution_count":13}],"source":["rf_model = RandomForestClassifier(n_estimators=100)\n","rf_model.fit(x_train_countvec, y_train)\n","pred_rf_model = rf_model.predict(x_test_countvec)\n","\n","print(\"----Count Vectorizer----\")\n","print(\"Accuracy {}\".format(accuracy_score(y_test, pred_rf_model)))\n","print(\"F1-score {}\".format(f1_score(y_test, pred_rf_model,average = \"macro\")))\n","print(confusion_matrix(y_test,pred_rf_model))\n","\n","rf_model = RandomForestClassifier(n_estimators=100)\n","rf_model.fit(x_train, y_train)\n","pred_rf_model = rf_model.predict(x_test)\n","\n","print(\"----TFIDF----\")\n","print(\"Accuracy {}\".format(accuracy_score(y_test, pred_rf_model)))\n","print(\"F1-score {}\".format(f1_score(y_test, pred_rf_model,average = \"macro\")))\n","confusion_matrix(y_test,pred_rf_model)"]},{"cell_type":"markdown","metadata":{"id":"msUpP9VmJQIQ"},"source":["# LSTM "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r5gTGWwoDvBr"},"outputs":[],"source":["vocab_size = 8000\n","max_len = 896"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aFEapIK1JVjN"},"outputs":[],"source":["#tokenization\n","tokenizer = Tokenizer(num_words = vocab_size)\n","tokenizer.fit_on_texts(df_train['clean_text'].values)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qt6YsyQOHLm7"},"outputs":[],"source":["train_encoding = sequence.pad_sequences(tokenizer.texts_to_sequences(df_train['clean_text'].values),maxlen = max_len)\n","test_encoding = sequence.pad_sequences(tokenizer.texts_to_sequences(df_test['clean_text'].values),maxlen = max_len)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vQ2neo4jKZSa"},"outputs":[],"source":["class comment_dataset(Dataset):\n","  def __init__(self,X,Y):\n","    self.X = X\n","    self.y = Y\n","  \n","  def __len__(self):\n","    return len(self.y)\n","\n","  def __getitem__(self,idx):\n","    return torch.tensor(self.X[idx]),self.y[idx]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OQ7jUgBELB54"},"outputs":[],"source":["batch_size = 32\n","\n","train_dataset = comment_dataset(train_encoding,df_train['class'].values)\n","test_dataset = comment_dataset(test_encoding,df_test['class'].values)\n","train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","test_dl = DataLoader(test_dataset, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CWB0f9SBP6yN"},"outputs":[],"source":["class LSTM_fixed_len(torch.nn.Module) :\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim) :\n","        super().__init__()\n","    \n","        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n","        self.linear = nn.Linear(hidden_dim, 5)\n","        self.dropout = nn.Dropout(0.2)\n","        \n","    def forward(self, x):\n","        x = self.embeddings(x)\n","        x = self.dropout(x)\n","        lstm_out, (ht, ct) = self.lstm(x)\n","        return self.linear(ht[-1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_0bxBT_4Qwf-"},"outputs":[],"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","model_fixed =  LSTM_fixed_len(vocab_size, 50, 50).to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r3t_t6htRAni"},"outputs":[],"source":["def train_model(model, epochs=10, lr=0.001):\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","    for i in range(epochs):\n","        model.train()\n","        sum_loss = 0.0\n","        total = 0\n","        for x, y in tqdm(train_dl):\n","            x = x.to(device)\n","            y = y.to(device)\n","            y_pred = model(x)\n","            y = y.cpu()\n","            y_pred = y_pred.cpu()\n","            loss = F.cross_entropy(y_pred, y)\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            sum_loss += loss.item()*y.shape[0]\n","            total += y.shape[0]\n","        test_loss, test_acc, test_rmse = validation_metrics(model, test_dl)\n","        print(\"train loss %.3f, test loss %.3f, test accuracy %.3f, and test rmse %.3f\" %(sum_loss/total, test_loss, test_acc, test_rmse))\n","\n","def validation_metrics (model, test_dl):\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    sum_loss = 0.0\n","    sum_rmse = 0.0\n","    for x, y in test_dl:\n","        x = x.to(device)\n","        y = y.to(device)\n","        y_hat = model(x)\n","        y_hat = y_hat.cpu()\n","        y = y.cpu()\n","        loss = F.cross_entropy(y_hat, y)\n","        pred = torch.max(y_hat, 1)[1]\n","        correct += (pred == y).float().sum()\n","        total += y.shape[0]\n","        sum_loss += loss.item()*y.shape[0]\n","        sum_rmse += np.sqrt(mean_squared_error(pred, y.unsqueeze(-1)))*y.shape[0]\n","    return sum_loss/total, correct/total, sum_rmse/total"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MWSAY8kTEqhZ","outputId":"5a568897-807a-43c3-c738-fa1b9f997a3a","executionInfo":{"status":"ok","timestamp":1650156207620,"user_tz":240,"elapsed":31869,"user":{"displayName":"Roshan Konda","userId":"04517896983970965571"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 156/156 [00:06<00:00, 24.93it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train loss 0.513, test loss 0.569, test accuracy 0.767, and test rmse 0.493\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 156/156 [00:05<00:00, 27.26it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train loss 0.384, test loss 0.574, test accuracy 0.800, and test rmse 0.451\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 156/156 [00:05<00:00, 26.33it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train loss 0.265, test loss 0.614, test accuracy 0.786, and test rmse 0.475\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 156/156 [00:05<00:00, 27.35it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train loss 0.203, test loss 0.749, test accuracy 0.779, and test rmse 0.485\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 156/156 [00:05<00:00, 27.46it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train loss 0.167, test loss 0.781, test accuracy 0.782, and test rmse 0.480\n"]}],"source":["train_model(model_fixed, epochs=5, lr=0.01)"]},{"cell_type":"markdown","metadata":{"id":"xAmz5QA7JWbg"},"source":["# Pretrained Models BERT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BoF5i5iZMDYu"},"outputs":[],"source":["class RedditDataset(Dataset):\n","\n","  def __init__(self, text, targets, tokenizer, max_len):\n","    self.text = text\n","    self.targets = targets\n","    self.tokenizer = tokenizer\n","    self.max_len = max_len\n","  \n","  def __len__(self):\n","    return len(self.text)\n","  \n","  def __getitem__(self, item):\n","    comment = self.text[item]\n","    target = self.targets[item]\n","\n","    encoding_body = self.tokenizer.encode_plus(\n","      comment,\n","      add_special_tokens=True,\n","      max_length=self.max_len,\n","      return_token_type_ids=False,\n","      pad_to_max_length=True,\n","      return_attention_mask=True,\n","      return_tensors='pt',\n","      truncation = True\n","    )\n","\n","\n","    return {\n","      'comment_body': comment,\n","      'input_ids': encoding_body['input_ids'].flatten(),\n","      'attention_mask': encoding_body['attention_mask'].flatten(),\n","      'targets': torch.tensor(target, dtype=torch.long)\n","    }\n","\n","def create_data_loader(df, tokenizer, max_len, batch_size):\n","  ds = RedditDataset(\n","    text=df.clean_text.to_numpy(),\n","    targets=df['class'].to_numpy(),\n","    tokenizer=tokenizer,\n","    max_len=max_len\n","  )\n","\n","  return DataLoader(\n","    ds,\n","    batch_size=batch_size,\n","    num_workers=2\n","  )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lhAJ51aARQr-","colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["3c29f3fb0569484c9733925773f2ff74","9e0c9a53471947cca81f3b2e63b63af9","7326288426304d69a950dab98ca8a9cc","c0fa4cb85cc44a22aadf9520f4b650ed","0cf4818f1a9546b989a1936f0939ed82","e13c3ffefbdf47f589606066403debea","bf447694f2d34544976d452713fc27cb","eed8e80c9624429395c1c1151501a730","2b0fd8ded675437cb4daff513a6e1adf","c024c53f8ac54694b592a4bd1903c4bc","458b32959bdf4273b06227bca6578461","27cb9eefdaf6436b99eb374adffb6d53","85d57854c73240c5ace68f40d2af4b97","7dc957ac454e42969ac94abf228e77ca","9fd193b6a00645a4b3103760f8ad50e1","70bb81ede7ad43f5820ab0629f811076","5d0e5c50cf2742ffaaeb36f36c5f0862","2b1dfddb51e243bf95c1100f2e44afe9","45a185df48074854ae205a726a96f019","d3e0f1b4370645b492982bd9ef413c68","5a8e6c3ebda84d55aad07a367b70660d","08d2e835a75c4f838e535e3d71481f8e","24ddf4e9e3564e47afe3698a8910505e","d61cda35f1a943dbbfed2cfec7d8da49","14acc6dfff2247ee8db14e1498e3186d","03f05d62f66647b28557eaa7033cab10","dd11b016b60e4ea39fe768fde28a8e75","c5a2795a6db2402c89ce5b907eb52f11","c72d6dfb2288494ca47cf860c78b924f","457f5f807f4d41a6993c2e7ed0ba8d76","a4596a4e1c4442b3a2711738ba5a1bb7","3d068c05ffe444868012fa1c793be135","11e4acb160c0408bad13bd7144bfd843"]},"executionInfo":{"status":"ok","timestamp":1650156285906,"user_tz":240,"elapsed":3429,"user":{"displayName":"Roshan Konda","userId":"04517896983970965571"}},"outputId":"6e0dc2df-116c-4319-d02a-bd69b03dbe04"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c29f3fb0569484c9733925773f2ff74"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27cb9eefdaf6436b99eb374adffb6d53"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24ddf4e9e3564e47afe3698a8910505e"}},"metadata":{}}],"source":["MAX_LEN = 512\n","BATCH_SIZE = 16\n","PRE_TRAINED_MODEL_NAME = 'bert-base-uncased'\n","tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n","train_dataloader = create_data_loader(df_train, tokenizer = tokenizer, max_len = MAX_LEN, batch_size=BATCH_SIZE)\n","test_dataloader = create_data_loader(df_test, tokenizer = tokenizer, max_len = MAX_LEN, batch_size=BATCH_SIZE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xlcRIju-RQ92"},"outputs":[],"source":["nb_labels = 3\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","class RedditClassifier(nn.Module):\n","  def __init__(self,nb_labels):\n","    super(RedditClassifier,self).__init__()\n","    self.bert_body = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n","    #self.bert_body = RobertaModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n","\n","\n","    self.drop = nn.Dropout(p=0.3)\n","    self.out = nn.Linear(self.bert_body.config.hidden_size, nb_labels)\n","\n","\n","  def forward(self, input_ids, attention_mask):\n","    _, pooled_output = self.bert_body(\n","        input_ids = input_ids,\n","        attention_mask = attention_mask,\n","        return_dict = False\n","    )\n","\n","    output = self.drop(pooled_output)\n","    return self.out(output)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ynU_ZNKETRXx","colab":{"base_uri":"https://localhost:8080/","height":121,"referenced_widgets":["93be7215508d493794fa10dd857d6561","149bbfd3f1f04a0184c32c84ef8955b3","f22a1588ede74e76a36f236cf5f5842c","71755d9cd40e466781d344694a38beeb","d1f3cc67da594e9fa4ad5b9506d35747","0776333f1a6f478a956dae4851819f78","fc925cc77bd3482890feebd6b3fab1b6","71a100a441fa402a8d041022d1331b90","cc3b08ada0774f3dbcfdb19b01074e72","ef428ea4be0446749a3bdcf0d91a2dff","16c1b4dba98f401494944e8ebc857022"]},"executionInfo":{"status":"ok","timestamp":1650156316959,"user_tz":240,"elapsed":13761,"user":{"displayName":"Roshan Konda","userId":"04517896983970965571"}},"outputId":"2f31843d-271b-4713-beb4-03024de71242"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93be7215508d493794fa10dd857d6561"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["EPOCHS = 1\n","model = RedditClassifier(nb_labels)\n","model.to(device)\n","optimizer = AdamW(model.parameters(), lr=2e-5)\n","total_steps = len(train_dataloader) * EPOCHS\n","\n","scheduler = get_linear_schedule_with_warmup(\n","  optimizer,\n","  num_warmup_steps=0,\n","  num_training_steps=total_steps\n",")\n","\n","loss_fn = nn.CrossEntropyLoss().to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UOFeQ8sf3czD"},"outputs":[],"source":["def train_epoch(\n","  model, \n","  data_loader, \n","  loss_fn, \n","  optimizer, \n","  device, \n","  scheduler, \n","  n_examples\n","):\n","  model = model.train()\n","\n","  losses = []\n","  correct_predictions = 0\n","  \n","  for d in tqdm(data_loader,position = 0,leave = True):\n","    input_ids = d[\"input_ids\"].to(device)\n","    attention_mask = d[\"attention_mask\"].to(device)\n","    targets = d[\"targets\"].to(device)\n","\n","\n","    outputs = model(\n","      input_ids=input_ids,\n","      attention_mask=attention_mask\n","    )\n","    _, preds = torch.max(outputs, dim=1)\n","    loss = loss_fn(outputs, targets)\n","\n","    correct_predictions += torch.sum(preds == targets)\n","    losses.append(loss.item())\n","\n","    loss.backward()\n","    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","    optimizer.step()\n","    scheduler.step()\n","    optimizer.zero_grad()\n","\n","  return correct_predictions.double() / n_examples, np.mean(losses)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"86P1SN_Z4wDp"},"outputs":[],"source":["def eval_model(model, data_loader, loss_fn, device, n_examples):\n","  model = model.eval()\n","\n","  losses = []\n","  correct_predictions = 0\n","\n","  with torch.no_grad():\n","    for d in data_loader:\n","      input_ids = d[\"input_ids\"].to(device)\n","      attention_mask = d[\"attention_mask\"].to(device)\n","      targets = d[\"targets\"].to(device)\n","\n","\n","\n","      outputs = model(\n","        input_ids=input_ids,\n","        attention_mask=attention_mask,\n","\n","      )\n","      x = outputs\n","      _, preds = torch.max(outputs, dim=1)\n","\n","      loss = loss_fn(outputs, targets)\n","\n","      correct_predictions += torch.sum(preds == targets)\n","      losses.append(loss.item())\n","\n","  return correct_predictions.double() / n_examples, np.mean(losses)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-9Z2WGiI7RCM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650156614177,"user_tz":240,"elapsed":292396,"user":{"displayName":"Roshan Konda","userId":"04517896983970965571"}},"outputId":"1d8064bf-61cd-4019-b1ce-6ac7287e7172"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/1\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/311 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","100%|██████████| 311/311 [04:28<00:00,  1.16it/s]"]},{"output_type":"stream","name":"stdout","text":["Train loss 0.604260244457668 accuracy 0.7625855819573097\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Test loss 0.4328461774648764 accuracy 0.8244766505636071\n","\n"]}],"source":["history = defaultdict(list)\n","best_accuracy = 0\n","\n","for epoch in range(EPOCHS):\n","\n","  print(f'Epoch {epoch + 1}/{EPOCHS}')\n","  print('-' * 10)\n","\n","  train_acc, train_loss = train_epoch(\n","    model,\n","    train_dataloader,    \n","    loss_fn, \n","    optimizer, \n","    device, \n","    scheduler, \n","    len(df_train)\n","  )\n","\n","  print(f'Train loss {train_loss} accuracy {train_acc}')\n","\n","  test_acc, test_loss = eval_model(\n","    model,\n","    test_dataloader,\n","    loss_fn, \n","    device, \n","    len(df_test)\n","  )\n","\n","  print(f'Test loss {test_loss} accuracy {test_acc}')\n","  print()\n","\n","  history['train_acc'].append(train_acc)\n","  history['train_loss'].append(train_loss)\n","  history['test_acc'].append(test_acc)\n","  history['test_loss'].append(test_loss)\n","\n","  if test_acc > best_accuracy:\n","    torch.save(model.state_dict(), 'best_model_state.bin')\n","    best_accuracy = test_acc"]},{"cell_type":"markdown","metadata":{"id":"tUx3hGHHFxla"},"source":["# Small-Text Active Learning Entropy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QJqw2E-TF0Wp"},"outputs":[],"source":["from small_text.integrations.transformers.datasets import TransformersDataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ViqAXOZqGL-r"},"outputs":[],"source":["from transformers import AutoTokenizer\n","\n","transformer_model_name = 'bert-base-uncased'\n","\n","tokenizer = AutoTokenizer.from_pretrained(\n","    transformer_model_name\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NuibZblkF8h6"},"outputs":[],"source":["def get_transformers_dataset(tokenizer, data, labels, max_length=60):\n","\n","    data_out = []\n","\n","    for i, doc in enumerate(data):\n","        encoded_dict = tokenizer.encode_plus(\n","            doc,\n","            add_special_tokens=True,\n","            padding='max_length',\n","            max_length= 512,\n","            return_attention_mask=True,\n","            return_tensors='pt',\n","            truncation='longest_first'\n","        )\n","\n","        data_out.append((encoded_dict['input_ids'], encoded_dict['attention_mask'], labels[i]))\n","\n","    return TransformersDataset(data_out)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9KkZuWXRGHBA"},"outputs":[],"source":["train = get_transformers_dataset(tokenizer,df_train['clean_text'].values,df_train['class'].values)\n","test = get_transformers_dataset(tokenizer,df_test['clean_text'].values,df_test['class'].values)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WSH2MB9oGb0k"},"outputs":[],"source":["from small_text.active_learner import PoolBasedActiveLearner\n","\n","from small_text.initialization import random_initialization_balanced\n","from small_text.integrations.transformers import TransformerModelArguments\n","from small_text.integrations.transformers.classifiers.factories import TransformerBasedClassificationFactory\n","from small_text.query_strategies import PredictionEntropy\n","from small_text.integrations.transformers import TransformerModelArguments\n","\n","def initialize_active_learner(active_learner, y_train):\n","\n","    indices_initial = random_initialization_balanced(y_train, n_samples=20)\n","    active_learner.initialize_data(indices_initial, y_train[indices_initial])\n","\n","    return indices_initial\n","\n","num_classes = 3\n","transformer_model = TransformerModelArguments(transformer_model_name)\n","clf_factory = TransformerBasedClassificationFactory(transformer_model, \n","                                                    num_classes, \n","                                                    kwargs=dict({'device': 'cuda', \n","                                                                 'mini_batch_size': 16,\n","                                                                 'class_weight': 'balanced'\n","                                                                }))\n","query_strategy = PredictionEntropy()\n","active_learner = PoolBasedActiveLearner(clf_factory, query_strategy, train)\n","indices_labeled = initialize_active_learner(active_learner, train.y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sktD_IY0G__m"},"outputs":[],"source":["from sklearn.metrics import accuracy_score\n","\n","\n","num_queries = 11\n","\n","\n","def evaluate(active_learner, train, test):\n","    y_pred = active_learner.classifier.predict(train)\n","    y_pred_test = active_learner.classifier.predict(test)\n","    \n","    test_acc = accuracy_score(y_pred_test, test.y)\n","\n","    print('Train accuracy: {:.2f}'.format(accuracy_score(y_pred, train.y)))\n","    print('Test accuracy: {:.2f}'.format(test_acc))\n","    \n","    return test_acc\n","\n","\n","results = []\n","results.append(evaluate(active_learner, train[indices_labeled], test))\n","indices_used = []\n","\n","    \n","for i in range(num_queries):\n","    # ...where each iteration consists of labelling 20 samples\n","    indices_queried = active_learner.query(num_samples=490)\n","    indices_used.append(indices_queried)\n","\n","\n","    # Simulate user interaction here. Replace this for real-world usage.\n","    y = train.y[indices_queried]\n","\n","    # Return the labels for the current query to the active learner.\n","    active_learner.update(y)\n","\n","    indices_labeled = np.concatenate([indices_queried, indices_labeled])\n","    \n","    print('---------------')\n","    print(f'Iteration #{i} ({len(indices_labeled)} samples)')\n","    results.append(evaluate(active_learner, train[indices_labeled], test))"]},{"cell_type":"markdown","source":["# Small Text Active Learning Least Confident"],"metadata":{"id":"beP-e4qY40-D"}},{"cell_type":"code","source":["from small_text.integrations.transformers.datasets import TransformersDataset"],"metadata":{"id":"qOeVFoB545A4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","\n","transformer_model_name = 'bert-base-uncased'\n","\n","tokenizer = AutoTokenizer.from_pretrained(\n","    transformer_model_name\n",")"],"metadata":{"id":"KnVkLptcFaFQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_transformers_dataset(tokenizer, data, labels, max_length=60):\n","\n","    data_out = []\n","\n","    for i, doc in enumerate(data):\n","        encoded_dict = tokenizer.encode_plus(\n","            doc,\n","            add_special_tokens=True,\n","            padding='max_length',\n","            max_length= 512,\n","            return_attention_mask=True,\n","            return_tensors='pt',\n","            truncation='longest_first'\n","        )\n","\n","        data_out.append((encoded_dict['input_ids'], encoded_dict['attention_mask'], labels[i]))\n","\n","    return TransformersDataset(data_out)"],"metadata":{"id":"a6N8QblVFhns"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train = get_transformers_dataset(tokenizer,df_train['clean_text'].values,df_train['class'].values)\n","test = get_transformers_dataset(tokenizer,df_test['clean_text'].values,df_test['class'].values)"],"metadata":{"id":"nUhPUIzyFlRO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from small_text.active_learner import PoolBasedActiveLearner\n","\n","from small_text.initialization import random_initialization_balanced\n","from small_text.integrations.transformers import TransformerModelArguments\n","from small_text.integrations.transformers.classifiers.factories import TransformerBasedClassificationFactory\n","from small_text.query_strategies import LeastConfidence\n","from small_text.integrations.transformers import TransformerModelArguments\n","\n","def initialize_active_learner(active_learner, y_train):\n","\n","    indices_initial = random_initialization_balanced(y_train, n_samples=20)\n","    active_learner.initialize_data(indices_initial, y_train[indices_initial])\n","\n","    return indices_initial\n","\n","num_classes = 3\n","transformer_model = TransformerModelArguments(transformer_model_name)\n","clf_factory = TransformerBasedClassificationFactory(transformer_model, \n","                                                    num_classes, \n","                                                    kwargs=dict({'device': 'cuda', \n","                                                                 'mini_batch_size': 14,\n","                                                                 'class_weight': 'balanced'\n","                                                                }))\n","query_strategy = LeastConfidence()\n","active_learner = PoolBasedActiveLearner(clf_factory, query_strategy, train)\n","indices_labeled = initialize_active_learner(active_learner, train.y)"],"metadata":{"id":"O-RAs5oHFrDn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score\n","\n","\n","num_queries = 11\n","\n","\n","def evaluate(active_learner, train, test):\n","    y_pred = active_learner.classifier.predict(train)\n","    y_pred_test = active_learner.classifier.predict(test)\n","    \n","    test_acc = accuracy_score(y_pred_test, test.y)\n","\n","    print('Train accuracy: {:.2f}'.format(accuracy_score(y_pred, train.y)))\n","    print('Test accuracy: {:.2f}'.format(test_acc))\n","    \n","    return test_acc\n","\n","\n","results = []\n","results.append(evaluate(active_learner, train[indices_labeled], test))\n","indices_used = []\n","\n","    \n","for i in range(num_queries):\n","    # ...where each iteration consists of labelling 20 samples\n","    indices_queried = active_learner.query(num_samples=490)\n","    indices_used.append(indices_queried)\n","\n","\n","    # Simulate user interaction here. Replace this for real-world usage.\n","    y = train.y[indices_queried]\n","\n","    # Return the labels for the current query to the active learner.\n","    active_learner.update(y)\n","\n","    indices_labeled = np.concatenate([indices_queried, indices_labeled])\n","    \n","    print('---------------') \n","    print(f'Iteration #{i} ({len(indices_labeled)} samples)')\n","    results.append(evaluate(active_learner, train[indices_labeled], test))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"PxpiPJTEFxy9","executionInfo":{"status":"error","timestamp":1649742434407,"user_tz":240,"elapsed":19946696,"user":{"displayName":"Roshan Konda","userId":"04517896983970965571"}},"outputId":"16bcec99-0666-4eb5-c8a5-82361ee9be33"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train accuracy: 0.50\n","Test accuracy: 0.20\n","---------------\n","Iteration #0 (510 samples)\n","Train accuracy: 0.86\n","Test accuracy: 0.61\n","---------------\n","Iteration #1 (1000 samples)\n","Train accuracy: 0.49\n","Test accuracy: 0.51\n","---------------\n","Iteration #2 (1490 samples)\n","Train accuracy: 0.77\n","Test accuracy: 0.66\n","---------------\n","Iteration #3 (1980 samples)\n","Train accuracy: 0.68\n","Test accuracy: 0.64\n","---------------\n","Iteration #4 (2470 samples)\n","Train accuracy: 0.35\n","Test accuracy: 0.47\n","---------------\n","Iteration #5 (2960 samples)\n","Train accuracy: 0.68\n","Test accuracy: 0.64\n","---------------\n","Iteration #6 (3450 samples)\n","Train accuracy: 0.70\n","Test accuracy: 0.64\n","---------------\n","Iteration #7 (3940 samples)\n","Train accuracy: 0.77\n","Test accuracy: 0.71\n","---------------\n","Iteration #8 (4430 samples)\n","Train accuracy: 0.80\n","Test accuracy: 0.67\n","---------------\n","Iteration #9 (4920 samples)\n","Train accuracy: 0.87\n","Test accuracy: 0.75\n"]},{"output_type":"error","ename":"PoolExhaustedException","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPoolExhaustedException\u001b[0m                    Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-01920e37140a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_queries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# ...where each iteration consists of labelling 20 samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mindices_queried\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactive_learner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m490\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mindices_used\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_queried\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/small_text/active_learner.py\u001b[0m in \u001b[0;36mquery\u001b[0;34m(self, num_samples, representation, query_strategy_kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m                                                          \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m                                                          \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m                                                          **query_strategy_kwargs)\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices_queried\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/small_text/query_strategies/strategies.py\u001b[0m in \u001b[0;36mquery\u001b[0;34m(self, clf, dataset, indices_unlabeled, indices_labeled, y, n)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_unlabeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_labeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_query_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_unlabeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mconfidence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_unlabeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_labeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/small_text/query_strategies/strategies.py\u001b[0m in \u001b[0;36m_validate_query_input\u001b[0;34m(indices_unlabeled, n)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_unlabeled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             raise PoolExhaustedException('Pool exhausted: {} available / {} requested'\n\u001b[0;32m---> 49\u001b[0;31m                                          .format(len(indices_unlabeled), n))\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mPoolExhaustedException\u001b[0m: Pool exhausted: 46 available / 490 requested"]}]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["3qIrp2kRqKzO","lEDGq28Vqw-Q","msUpP9VmJQIQ","tUx3hGHHFxla","beP-e4qY40-D"],"name":"Copy of ruddit.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"3c29f3fb0569484c9733925773f2ff74":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9e0c9a53471947cca81f3b2e63b63af9","IPY_MODEL_7326288426304d69a950dab98ca8a9cc","IPY_MODEL_c0fa4cb85cc44a22aadf9520f4b650ed"],"layout":"IPY_MODEL_0cf4818f1a9546b989a1936f0939ed82"}},"9e0c9a53471947cca81f3b2e63b63af9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e13c3ffefbdf47f589606066403debea","placeholder":"​","style":"IPY_MODEL_bf447694f2d34544976d452713fc27cb","value":"Downloading: 100%"}},"7326288426304d69a950dab98ca8a9cc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_eed8e80c9624429395c1c1151501a730","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2b0fd8ded675437cb4daff513a6e1adf","value":231508}},"c0fa4cb85cc44a22aadf9520f4b650ed":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c024c53f8ac54694b592a4bd1903c4bc","placeholder":"​","style":"IPY_MODEL_458b32959bdf4273b06227bca6578461","value":" 226k/226k [00:00&lt;00:00, 691kB/s]"}},"0cf4818f1a9546b989a1936f0939ed82":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e13c3ffefbdf47f589606066403debea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bf447694f2d34544976d452713fc27cb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eed8e80c9624429395c1c1151501a730":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b0fd8ded675437cb4daff513a6e1adf":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c024c53f8ac54694b592a4bd1903c4bc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"458b32959bdf4273b06227bca6578461":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"27cb9eefdaf6436b99eb374adffb6d53":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_85d57854c73240c5ace68f40d2af4b97","IPY_MODEL_7dc957ac454e42969ac94abf228e77ca","IPY_MODEL_9fd193b6a00645a4b3103760f8ad50e1"],"layout":"IPY_MODEL_70bb81ede7ad43f5820ab0629f811076"}},"85d57854c73240c5ace68f40d2af4b97":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5d0e5c50cf2742ffaaeb36f36c5f0862","placeholder":"​","style":"IPY_MODEL_2b1dfddb51e243bf95c1100f2e44afe9","value":"Downloading: 100%"}},"7dc957ac454e42969ac94abf228e77ca":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_45a185df48074854ae205a726a96f019","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d3e0f1b4370645b492982bd9ef413c68","value":28}},"9fd193b6a00645a4b3103760f8ad50e1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a8e6c3ebda84d55aad07a367b70660d","placeholder":"​","style":"IPY_MODEL_08d2e835a75c4f838e535e3d71481f8e","value":" 28.0/28.0 [00:00&lt;00:00, 791B/s]"}},"70bb81ede7ad43f5820ab0629f811076":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5d0e5c50cf2742ffaaeb36f36c5f0862":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b1dfddb51e243bf95c1100f2e44afe9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"45a185df48074854ae205a726a96f019":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d3e0f1b4370645b492982bd9ef413c68":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5a8e6c3ebda84d55aad07a367b70660d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"08d2e835a75c4f838e535e3d71481f8e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"24ddf4e9e3564e47afe3698a8910505e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d61cda35f1a943dbbfed2cfec7d8da49","IPY_MODEL_14acc6dfff2247ee8db14e1498e3186d","IPY_MODEL_03f05d62f66647b28557eaa7033cab10"],"layout":"IPY_MODEL_dd11b016b60e4ea39fe768fde28a8e75"}},"d61cda35f1a943dbbfed2cfec7d8da49":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c5a2795a6db2402c89ce5b907eb52f11","placeholder":"​","style":"IPY_MODEL_c72d6dfb2288494ca47cf860c78b924f","value":"Downloading: 100%"}},"14acc6dfff2247ee8db14e1498e3186d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_457f5f807f4d41a6993c2e7ed0ba8d76","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a4596a4e1c4442b3a2711738ba5a1bb7","value":570}},"03f05d62f66647b28557eaa7033cab10":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3d068c05ffe444868012fa1c793be135","placeholder":"​","style":"IPY_MODEL_11e4acb160c0408bad13bd7144bfd843","value":" 570/570 [00:00&lt;00:00, 16.8kB/s]"}},"dd11b016b60e4ea39fe768fde28a8e75":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c5a2795a6db2402c89ce5b907eb52f11":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c72d6dfb2288494ca47cf860c78b924f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"457f5f807f4d41a6993c2e7ed0ba8d76":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a4596a4e1c4442b3a2711738ba5a1bb7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3d068c05ffe444868012fa1c793be135":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"11e4acb160c0408bad13bd7144bfd843":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"93be7215508d493794fa10dd857d6561":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_149bbfd3f1f04a0184c32c84ef8955b3","IPY_MODEL_f22a1588ede74e76a36f236cf5f5842c","IPY_MODEL_71755d9cd40e466781d344694a38beeb"],"layout":"IPY_MODEL_d1f3cc67da594e9fa4ad5b9506d35747"}},"149bbfd3f1f04a0184c32c84ef8955b3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0776333f1a6f478a956dae4851819f78","placeholder":"​","style":"IPY_MODEL_fc925cc77bd3482890feebd6b3fab1b6","value":"Downloading: 100%"}},"f22a1588ede74e76a36f236cf5f5842c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_71a100a441fa402a8d041022d1331b90","max":440473133,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cc3b08ada0774f3dbcfdb19b01074e72","value":440473133}},"71755d9cd40e466781d344694a38beeb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ef428ea4be0446749a3bdcf0d91a2dff","placeholder":"​","style":"IPY_MODEL_16c1b4dba98f401494944e8ebc857022","value":" 420M/420M [00:08&lt;00:00, 55.8MB/s]"}},"d1f3cc67da594e9fa4ad5b9506d35747":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0776333f1a6f478a956dae4851819f78":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fc925cc77bd3482890feebd6b3fab1b6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"71a100a441fa402a8d041022d1331b90":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc3b08ada0774f3dbcfdb19b01074e72":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ef428ea4be0446749a3bdcf0d91a2dff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"16c1b4dba98f401494944e8ebc857022":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}