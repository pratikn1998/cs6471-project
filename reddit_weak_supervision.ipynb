{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Gh9n9k1cE5Lw"},"outputs":[],"source":["!pip install transformers snorkel better_profanity textblob "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1e9DOTPLE9VM"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from google.colab import drive\n","from nltk.corpus import stopwords\n","import nltk\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import SVC\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","import tensorflow_hub as hub\n","from transformers import BertTokenizer\n","from transformers import BertTokenizerFast, BertForSequenceClassification\n","from transformers import BertConfig\n","from transformers import BertModel, get_linear_schedule_with_warmup\n","from transformers import RobertaTokenizer\n","from transformers import RobertaConfig\n","from transformers import RobertaModel\n","from torch.utils.data import Dataset, DataLoader\n","import torch\n","from torch import nn\n","from sklearn.metrics import accuracy_score,f1_score\n","from sklearn.metrics import confusion_matrix\n","from collections import Counter\n","import spacy\n","from keras.preprocessing import sequence\n","from keras.preprocessing.text import Tokenizer\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn.functional as F\n","from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","from keras.preprocessing import sequence\n","from tqdm import tqdm\n","from sklearn.metrics import mean_squared_error\n","from torch.optim import AdamW\n","from collections import defaultdict\n","from scipy.stats import entropy\n","from snorkel.labeling import labeling_function,PandasLFApplier\n","from snorkel.preprocess import preprocessor\n","from snorkel.preprocess.nlp import SpacyPreprocessor\n","from better_profanity import profanity\n","from textblob import TextBlob\n","from snorkel.labeling import LFAnalysis\n","from snorkel.analysis import get_label_buckets\n","from snorkel.labeling import LabelingFunction\n","from snorkel.labeling.model import MajorityLabelVoter\n","from snorkel.labeling.model import LabelModel\n","from snorkel.labeling import filter_unlabeled_dataframe\n","from snorkel.utils import probs_to_preds\n","from snorkel.preprocess.nlp import SpacyPreprocessor\n","import spacy\n","import random"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_tGUpZ3nFNfS"},"outputs":[],"source":["nltk.download('stopwords')\n","stop_words = set(stopwords.words('english'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ohP7kFVWFhtM"},"outputs":[],"source":["drive.mount('/content/gdrive')"]},{"cell_type":"code","source":["using_colab = True"],"metadata":{"id":"JpWcjJS4JuJJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RjUJQgTATNcW"},"source":["##Weak Supervison\n"]},{"cell_type":"code","source":["if using_colab:\n","  df = pd.read_csv('/content/gdrive/MyDrive/CS6471/reddit_comments.csv')\n","else:\n","  df = pd.read_csv('./reddit_comments.csv')\n"],"metadata":{"id":"NZ9_YVZN9ydg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#clean text\n","whitelist = set('abcdefghijklmnopqrstuvwxyz ABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890')\n","def cleaner(x):\n","    x = x.replace(\"\\n\",\" \")\n","    x = \"\".join(filter(whitelist.__contains__, x))\n","    x = x.lower()\n","    x = x.strip()\n","    return x"],"metadata":{"id":"xgKxNfLC-pis"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df[\"clean_text\"] = df[\"txt\"].apply(cleaner)"],"metadata":{"id":"6KgqssZF-sur"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"87_oTyYJGjtE"},"outputs":[],"source":["TOXIC = 1\n","NEUTRAL = -1\n","NONTOXIC = 0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a-rmpMLEGmUQ"},"outputs":[],"source":["@labeling_function()\n","def contains_bad_words(x):\n","    return TOXIC if profanity.contains_profanity(x.txt) else NEUTRAL"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rq_DZUaYGu_H"},"outputs":[],"source":["@preprocessor(memoize=True)\n","def comment_sentiment(x):\n","    scores = TextBlob(x.txt)\n","    x.polarity = scores.sentiment.polarity\n","    x.subjectivity = scores.sentiment.subjectivity\n","    return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cluTfn_jG6Ju"},"outputs":[],"source":["#if polarity is >.9 more likely to be more non-toxic\n","@labeling_function(pre=[comment_sentiment])\n","def comment_polarity(x):\n","    return NONTOXIC if x.polarity > 0.9 else NEUTRAL"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TOJIC5m0G7sE"},"outputs":[],"source":["#if subjectivity is >.7 more likely to be non-toxic\n","@labeling_function(pre=[comment_sentiment])\n","def comment_subjectivity(x):\n","    return NONTOXIC if x.subjectivity > 0.7 else NEUTRAL"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ljo-5I3MG_hZ"},"outputs":[],"source":["spacy = SpacyPreprocessor(text_field=\"txt\", doc_field=\"doc\", memoize=True)\n","#if comment contains URL more likely to be informative\n","@labeling_function(pre=[spacy])\n","def contains_url(x):\n","    \"\"\"If comment contains url, label non-toxic, else abstain\"\"\"\n","    matcher = Matcher(nlp.vocab)\n","    pattern = [{\"LIKE_URL\": True}]\n","    matcher.add(\"p1\", None, pattern)\n","    matches = matcher(x.doc)\n","    return NONTOXIC if len(matches)>0 else NEUTRAL"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z01ZKog9Qh-E"},"outputs":[],"source":["#checks to see if toxic and nontoxic phrases are present.\n","#Got this from https://trishalaneeraj.github.io/2020-07-26/data-labeling-weak-supervision\n","def keyword_lookup(x, keywords, label):\n","    if any(word in x.clean_text.lower() for word in keywords):\n","        return label\n","    return NEUTRAL\n","\n","def make_keyword_lf(keywords, label=TOXIC):\n","    return LabelingFunction(\n","        name=f\"keyword_{keywords[0]}\",\n","        f=keyword_lookup,\n","        resources=dict(keywords=keywords, label=label),\n","    )\n","    \n","with open('/content/gdrive/MyDrive/CS6471/badwords.txt') as f:\n","    toxic_stopwords = f.readlines()\n","\n","toxic_stopwords = [x.strip() for x in toxic_stopwords] # len = 458\n","\"\"\"Comments mentioning at least one of Google's Toxic Stopwords \n","https://code.google.com/archive/p/badwordslist/downloads are likely toxic\"\"\"\n","#Chceks to see if comments are toxic.\n","keyword_toxic_stopwords = make_keyword_lf(keywords=toxic_stopwords, label=TOXIC)\n","\n","#Looks for keywords of please\n","keyword_pl = make_keyword_lf(keywords=[\"please\", \"plz\", \"pls\", \"pl\"], label=NONTOXIC)\n","#Looks for keywords of thanks\n","keyword_thanks = make_keyword_lf(keywords=[\"thanks\", \"thank you\", \"thx\", \"tx\"], label=NONTOXIC)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hS53ehBRHB20"},"outputs":[],"source":["lfs = [contains_bad_words, comment_polarity, comment_subjectivity, keyword_toxic_stopwords,keyword_pl,keyword_thanks]\n","\n","applier = PandasLFApplier(lfs=lfs)\n","L_train = applier.apply(df)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ewaWhUjFWjlu"},"outputs":[],"source":["LFAnalysis(L=L_train, lfs=lfs).lf_summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"imtdRW9aoAa4"},"outputs":[],"source":["#Majority vote for prediction - probably poor model\n","majority_model = MajorityLabelVoter()\n","preds_train = majority_model.predict(L=L_train)\n","preds_train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FXjmQAE8oRy5"},"outputs":[],"source":["#More advanced model labeling model that will fit it based on true labels. How do we convert the output from snorkel to fit the true class labels?\n","label_model = LabelModel(cardinality=3, verbose=True)\n","\n","true_labels = df[\"class\"].to_numpy()\n","label_model.fit(L_train=L_train, n_epochs=1000, log_freq=200, seed=123)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2gjgWsFvEUMJ"},"outputs":[],"source":["majority_acc = majority_model.score(L=L_train, Y=true_labels, tie_break_policy=\"abstain\")[\n","    \"accuracy\"\n","]\n","print(f\"{'Majority Vote Accuracy:':<25} {majority_acc * 100:.1f}%\")\n","\n","label_model_acc = label_model.score(L=L_train, Y=true_labels, tie_break_policy=\"abstain\")[\n","    \"accuracy\"\n","]\n","print(f\"{'Label Model Accuracy:':<25} {label_model_acc * 100:.1f}%\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gt1jYZCEuG4q"},"outputs":[],"source":["label_model_weights = np.around(label_model.get_weights(), 3)\n","probs_train = np.asarray(label_model.predict_proba(L_train))\n","preds = probs_to_preds(probs=probs_train)\n","filtered_df, probs_train_filtered = filter_unlabeled_dataframe(\n","    X=df, y=probs_train, L=L_train\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Iv_6WZtRFteW"},"outputs":[],"source":["filtered_df = filtered_df[['post_id', 'txt','clean_text', 'class']]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vOIk0X-JWciv"},"outputs":[],"source":["filtered_df.loc[filtered_df[\"class\"] == -1, 'class'] = 'neutral'\n","filtered_df.loc[filtered_df[\"class\"] == 0, 'class'] = 'nontoxic'\n","filtered_df.loc[filtered_df[\"class\"] == 1, 'class'] = 'toxic'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0ekm3-GYW-3O"},"outputs":[],"source":["filtered_df.loc[filtered_df[\"class\"] ==  'neutral', 'class'] = 1\n","filtered_df.loc[filtered_df[\"class\"] == 'nontoxic', 'class'] = 0\n","filtered_df.loc[filtered_df[\"class\"] == 'toxic', 'class'] = 2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I-uSSKy6VLd4"},"outputs":[],"source":["filename = 'reddit_weak_sup_dat.csv'\n","if using_colab:\n","  filtered_df.to_csv(f\"/content/gdrive/MyDrive/CS6471/{filename}\", index=False)\n","else:\n","  filtered_df.to_csv(f\"/{filename}\", index=False)"]},{"cell_type":"markdown","metadata":{"id":"NFvaQxh1UBXy"},"source":["##Data Split for experiments 1,2,3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jpJE5066XmzB"},"outputs":[],"source":["if using_colab:\n","  df = pd.read_csv('/content/gdrive/MyDrive/CS6471/reddit_weak_sup_dat.csv')\n","else:\n","  df = pd.read_csv('./reddit_weak_sup_dat.csv')"]},{"cell_type":"code","source":["if using_colab: \n","  actualdf = pd.read_csv('/content/gdrive/MyDrive/CS6471/reddit_comments.csv')\n","else:\n","  df = pd.read_csv('./reddit_comments.csv')"],"metadata":{"id":"G7l12rC9IhLj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def discretize(val,split):\n","  if(split[0][0]<=val<split[0][1]):\n","    return 0\n","  elif(split[1][0]<=val<split[1][1]):\n","    return -1\n","  else:\n","    return 1\n","split = [\n","    [min(df['offensiveness_score']),-0.33],\n","    [-0.33,0.33],\n","    [0.33,max(df['offensiveness_score'])]\n","    ]"],"metadata":{"id":"h5fetnQX49pl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["actualdf['class'] = actualdf['offensiveness_score'].apply(lambda x:discretize(x,split))"],"metadata":{"id":"pmFZ6NkjgBuc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["actualdf.loc[actualdf['class'] == 1, 'class'] = 2\n","actualdf.loc[actualdf['class'] == -1, 'class'] =1\n","actualdf.loc[actualdf['class'] == 0, 'class'] = 0\n"],"metadata":{"id":"7-XV2vChgYzF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = df.rename(columns={'class': 'predicted_toxic'})"],"metadata":{"id":"tytxmClkIfbU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = df[['comment_id','predicted_toxic' ]]"],"metadata":{"id":"X4zHCFuPIeNA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dfe = pd.merge(df, actualdf, how='left', on=['comment_id','comment_id'], indicator=True)"],"metadata":{"id":"rgSnexrhIc9L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Experiment 1: Randomly fill nan values based on class distribution\n","###RUN ONLY FOR EXPERIMENT 1"],"metadata":{"id":"vjgkpJzL6CDM"}},{"cell_type":"code","source":["dfe['predicted_toxic'] = dfe['predicted_toxic'].fillna(5,inplace=True)\n","dfe[\"predicted_toxic\"]=dfe[\"predicted_toxic\"].apply(lambda x: random.randint(0,2) if x!=0 else x)"],"metadata":{"id":"gdZ-AwXV9PaC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Experiment 2: Drop unlabled data points\n","###RUN ONLY FOR EXPERIMENT 2"],"metadata":{"id":"8mdBNuyk6OU2"}},{"cell_type":"code","source":["dfe = dfe.dropna()"],"metadata":{"id":"3xigci3f6YTo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Experiment 3: Fill unlabled with true labels\n","###RUN ONLY FOR EXPERIMENT 3"],"metadata":{"id":"GVlslZfj6T9F"}},{"cell_type":"code","source":["dfe['predicted_toxic'].fillna(dfe['class'], inplace=True)"],"metadata":{"id":"2my5_UuK6aAW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Split Weak Supervised Data\n","###Continue to run here after picking experiment number"],"metadata":{"id":"VE_2Zmvt6duE"}},{"cell_type":"code","source":["#clean text\n","whitelist = set('abcdefghijklmnopqrstuvwxyz ABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890')\n","def cleaner(x):\n","    x = x.replace(\"\\n\",\" \")\n","    x = \"\".join(filter(whitelist.__contains__, x))\n","    x = x.lower()\n","    x = x.strip()\n","    return x"],"metadata":{"id":"rhzIW-jt7p6S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dfe[\"clean_text\"] = dfe[\"txt\"].apply(cleaner)"],"metadata":{"id":"66hmgNn6flnH"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mHPD70rrUDly"},"outputs":[],"source":["df_train, df_test = train_test_split(dfe, test_size=0.2, random_state=42)\n","y_train = list(df_train['predicted_toxic'])\n","y_test = list(df_test['class'])\n"]},{"cell_type":"markdown","metadata":{"id":"ws1vtJubUoDO"},"source":["# LSTM "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PN3j0siGUlqe"},"outputs":[],"source":["vocab_size = 8000\n","max_len = 896"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Lz7kCkgUqbP"},"outputs":[],"source":["#tokenization\n","tokenizer = Tokenizer(num_words = vocab_size)\n","tokenizer.fit_on_texts(df_train['clean_text'].values)#tokenization\n","tokenizer = Tokenizer(num_words = vocab_size)\n","tokenizer.fit_on_texts(df_train['clean_text'].values)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CC4FI6f3UttU"},"outputs":[],"source":["train_encoding = sequence.pad_sequences(tokenizer.texts_to_sequences(df_train['clean_text'].values),maxlen = max_len)\n","test_encoding = sequence.pad_sequences(tokenizer.texts_to_sequences(df_test['clean_text'].values),maxlen = max_len)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jQgaZLqpUu8O"},"outputs":[],"source":["class comment_dataset(Dataset):\n","  def __init__(self,X,Y):\n","    self.X = X\n","    self.y = Y\n","  \n","  def __len__(self):\n","    return len(self.y)\n","\n","  def __getitem__(self,idx):\n","    return torch.tensor(self.X[idx]),self.y[idx]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2pCL6zbuUw8h"},"outputs":[],"source":["batch_size = 32\n","\n","train_dataset = comment_dataset(train_encoding,df_train['predicted_toxic'].values)\n","test_dataset = comment_dataset(test_encoding,df_test['class'].values)\n","train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","test_dl = DataLoader(test_dataset, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QvO2nNe0UyzP"},"outputs":[],"source":["class LSTM_fixed_len(torch.nn.Module) :\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim) :\n","        super().__init__()\n","    \n","        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n","        self.linear = nn.Linear(hidden_dim, 5)\n","        self.dropout = nn.Dropout(0.2)\n","        \n","    def forward(self, x):\n","        x = self.embeddings(x)\n","        x = self.dropout(x)\n","        lstm_out, (ht, ct) = self.lstm(x)\n","        return self.linear(ht[-1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aZqFfhnwU0jw"},"outputs":[],"source":["model_fixed =  LSTM_fixed_len(vocab_size, 50, 50)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gtdpCFwnU4uN"},"outputs":[],"source":["def train_model(model, epochs=10, lr=0.001):\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","    for i in range(epochs):\n","        model.train()\n","        sum_loss = 0.0\n","        total = 0\n","        for x, y in tqdm(train_dl):\n","            y_pred = model(x)\n","            loss = F.cross_entropy(y_pred, y)\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            sum_loss += loss.item()*y.shape[0]\n","            total += y.shape[0]\n","        test_loss, test_acc, test_rmse = validation_metrics(model, test_dl)\n","        print(\"train loss %.3f, test loss %.3f, test accuracy %.3f, and test rmse %.3f\" %(sum_loss/total, test_loss, test_acc, test_rmse))\n","\n","def validation_metrics (model, test_dl):\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    sum_loss = 0.0\n","    sum_rmse = 0.0\n","    for x, y in test_dl:\n","        x = x.long()\n","        y = y.long()\n","        y_hat = model(x)\n","        loss = F.cross_entropy(y_hat, y)\n","        pred = torch.max(y_hat, 1)[1]\n","        correct += (pred == y).float().sum()\n","        total += y.shape[0]\n","        sum_loss += loss.item()*y.shape[0]\n","        sum_rmse += np.sqrt(mean_squared_error(pred, y.unsqueeze(-1)))*y.shape[0]\n","    return sum_loss/total, correct/total, sum_rmse/total"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HTqUe7Y_U_k6"},"outputs":[],"source":["train_model(model_fixed, epochs=3, lr=0.01)"]},{"cell_type":"markdown","metadata":{"id":"PLYu7esaY_HJ"},"source":["# Pretrained Models BERT, RoBERTA\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3NPapCGMZBim"},"outputs":[],"source":["class RedditDataset(Dataset):\n","\n","  def __init__(self, text, targets, tokenizer, max_len):\n","    self.text = text\n","    self.targets = targets\n","    self.tokenizer = tokenizer\n","    self.max_len = max_len\n","  \n","  def __len__(self):\n","    return len(self.text)\n","  \n","  def __getitem__(self, item):\n","    comment = self.text[item]\n","    target = self.targets[item]\n","\n","    encoding_body = self.tokenizer.encode_plus(\n","      comment,\n","      add_special_tokens=True,\n","      max_length=self.max_len,\n","      return_token_type_ids=False,\n","      pad_to_max_length=True,\n","      return_attention_mask=True,\n","      return_tensors='pt',\n","      truncation = True\n","    )\n","\n","\n","    return {\n","      'comment_body': comment,\n","      'input_ids': encoding_body['input_ids'].flatten(),\n","      'attention_mask': encoding_body['attention_mask'].flatten(),\n","      'targets': torch.tensor(target, dtype=torch.long)\n","    }\n","\n","def create_data_loader(df, tokenizer, max_len, batch_size, test_val):\n","  if test_val:\n","    test_class_name = 'class'\n","  else:\n","    test_class_name = 'predicted_toxic'\n","  ds = RedditDataset(\n","    text=df.clean_text.to_numpy(),\n","    targets=df[test_class_name].to_numpy(),\n","    tokenizer=tokenizer,\n","    max_len=max_len\n","  )\n","\n","  return DataLoader(\n","    ds,\n","    batch_size=batch_size,\n","    num_workers=2\n","  )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IP1ncUFYZElf"},"outputs":[],"source":["MAX_LEN = 512\n","BATCH_SIZE = 6\n","PRE_TRAINED_MODEL_NAME = 'bert-base-uncased'\n","tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n","train_dataloader = create_data_loader(df_train, tokenizer = tokenizer, max_len = MAX_LEN, batch_size=BATCH_SIZE, test_val=False)\n","test_dataloader = create_data_loader(df_test, tokenizer = tokenizer, max_len = MAX_LEN, batch_size=BATCH_SIZE, test_val=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nBNOlefsZGcD"},"outputs":[],"source":["nb_labels = 3\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","class RedditClassifier(nn.Module):\n","  def __init__(self,nb_labels):\n","    super(RedditClassifier,self).__init__()\n","    self.bert_body = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n","    #self.bert_body = RobertaModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n","\n","\n","    self.drop = nn.Dropout(p=0.3)\n","    self.out = nn.Linear(self.bert_body.config.hidden_size, nb_labels)\n","\n","\n","  def forward(self, input_ids, attention_mask):\n","    _, pooled_output = self.bert_body(\n","        input_ids = input_ids,\n","        attention_mask = attention_mask,\n","        return_dict = False\n","    )\n","\n","    output = self.drop(pooled_output)\n","    return self.out(output)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Srb-s2k4ZH3W"},"outputs":[],"source":["EPOCHS = 1\n","model = RedditClassifier(nb_labels)\n","model.to(device)\n","optimizer = AdamW(model.parameters(), lr=2e-5)\n","total_steps = len(train_dataloader) * EPOCHS\n","\n","scheduler = get_linear_schedule_with_warmup(\n","  optimizer,\n","  num_warmup_steps=0,\n","  num_training_steps=total_steps\n",")\n","\n","loss_fn = nn.CrossEntropyLoss().to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lWSLGxaGZKDq"},"outputs":[],"source":["def train_epoch(\n","  model, \n","  data_loader, \n","  loss_fn, \n","  optimizer, \n","  device, \n","  scheduler, \n","  n_examples\n","):\n","  model = model.train()\n","\n","  losses = []\n","  correct_predictions = 0\n","  \n","  for d in tqdm(data_loader,position = 0,leave = True):\n","    input_ids = d[\"input_ids\"].to(device)\n","    attention_mask = d[\"attention_mask\"].to(device)\n","    targets = d[\"targets\"].to(device)\n","\n","\n","    outputs = model(\n","      input_ids=input_ids,\n","      attention_mask=attention_mask\n","    )\n","    _, preds = torch.max(outputs, dim=1)\n","    loss = loss_fn(outputs, targets)\n","\n","    correct_predictions += torch.sum(preds == targets)\n","    losses.append(loss.item())\n","\n","    loss.backward()\n","    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","    optimizer.step()\n","    scheduler.step()\n","    optimizer.zero_grad()\n","\n","  return correct_predictions.double() / n_examples, np.mean(losses)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kb-ixjODZLt7"},"outputs":[],"source":["def eval_model(model, data_loader, loss_fn, device, n_examples):\n","  model = model.eval()\n","\n","  losses = []\n","  correct_predictions = 0\n","\n","  with torch.no_grad():\n","    for d in data_loader:\n","      input_ids = d[\"input_ids\"].to(device)\n","      attention_mask = d[\"attention_mask\"].to(device)\n","      targets = d[\"targets\"].to(device)\n","\n","\n","\n","      outputs = model(\n","        input_ids=input_ids,\n","        attention_mask=attention_mask,\n","\n","      )\n","      x = outputs\n","      _, preds = torch.max(outputs, dim=1)\n","\n","      loss = loss_fn(outputs, targets)\n","\n","      correct_predictions += torch.sum(preds == targets)\n","      losses.append(loss.item())\n","\n","  return correct_predictions.double() / n_examples, np.mean(losses)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zxT_BF5mZNtk"},"outputs":[],"source":["history = defaultdict(list)\n","best_accuracy = 0\n","\n","for epoch in range(EPOCHS):\n","\n","  print(f'Epoch {epoch + 1}/{EPOCHS}')\n","  print('-' * 10)\n","\n","  train_acc, train_loss = train_epoch(\n","    model,\n","    train_dataloader,    \n","    loss_fn, \n","    optimizer, \n","    device, \n","    scheduler, \n","    len(df_train)\n","  )\n","\n","  print(f'Train loss {train_loss} accuracy {train_acc}')\n","\n","  test_acc, test_loss = eval_model(\n","    model,\n","    test_dataloader,\n","    loss_fn, \n","    device, \n","    len(df_test)\n","  )\n","\n","  print(f'Test loss {test_loss} accuracy {test_acc}')\n","  print()\n","\n","  history['train_acc'].append(train_acc)\n","  history['train_loss'].append(train_loss)\n","  history['test_acc'].append(test_acc)\n","  history['test_loss'].append(test_loss)\n","\n","  if test_acc > best_accuracy:\n","    torch.save(model.state_dict(), 'best_model_state.bin')\n","    best_accuracy = test_acc"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["vjgkpJzL6CDM","8mdBNuyk6OU2","GVlslZfj6T9F","VE_2Zmvt6duE","ws1vtJubUoDO"],"name":"reddit_weak_supervision.ipynb","provenance":[{"file_id":"1EsKYeh9lTYJu0W8RoyYYWuCLuYbgt6Qu","timestamp":1651026543241}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}